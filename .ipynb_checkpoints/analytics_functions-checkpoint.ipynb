{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####\n",
    "# this is just an example. The function is not used\n",
    "#def feature_rescaling_minmax(item, minval, maxval):\n",
    "    # feature rescaling based on min/max values:\n",
    "    # x' = (x-min(x))/(max(x)-mix(x))\n",
    "    # example for apply \n",
    "    # df['new'] = df[i].apply(lambda x: myfunc(x, arg1 , arg2 ))\n",
    "#    val = (float(item)-float(minval))/(float(maxval)-float(minval))\n",
    "#    return val\n",
    "###\n",
    "\n",
    "def feature_rescaling_minmax(df, col_name):\n",
    "    # Function applies minmax scaling: x' = (x-min(x))/(max(x)-mix(x))\n",
    "    # input: data-frame, and target column to rescale (assumes column is numeric)\n",
    "    # returns: scaled column, cast to float\n",
    "    x = (pd.to_numeric(df[col_name]) - float(min(df[col_name])))/( float(max(df[col_name]))- float(min(df[col_name])))\n",
    "    return x\n",
    "    \n",
    "def feature_standardization(df, col_name):\n",
    "    # Function applies minmax scaling: x' = (x-min(x))/(max(x)-mix(x))\n",
    "    # input: data-frame, and target column to rescale (assumes column is numeric)\n",
    "    # returns: standardized column, cast to float\n",
    "    std = float(df[col_name].std())\n",
    "    mean = float(df[col_name].mean())\n",
    "    x = (df[col_name]-mean)/(std)\n",
    "    return x   \n",
    "    \n",
    "\n",
    "def value_pairs_to_df(colname_A, colname_B, colvalues_A, colvalues_B, toabs=False):\n",
    "    # expects: (array, array, str, str)\n",
    "    # returns: sorted df by colvalues_B, descending \n",
    "    # toabs==True: sort by absolute value \n",
    "    # converts two lists, A and B, to dataframe\n",
    "    df = pd.DataFrame(data=[colvalues_A, colvalues_B]).transpose()\n",
    "    df.columns = [colname_A, colname_B]\n",
    "\n",
    "    #df.sort_values(colname_B, ascending=[0], inplace=True)\n",
    "    #df.reset_index(inplace=True, drop=True)\n",
    "    if toabs: # sort by absolute value\n",
    "        #df = df.reindex(df[colname_B].abs().sort_values(inplace=False, ascending=0).index)\n",
    "        df = df.reindex(df[colname_B].abs().sort_values(inplace=False, ascending=False).index)\n",
    "    else:     # sort by value\n",
    "        df.sort_values(colname_B, ascending=[0], inplace=True)\n",
    "        df.reset_index(inplace=True, drop=True)\n",
    "        \n",
    "    return df\n",
    "\n",
    "def train_test_unbalanced_dataset(df, col_class, frac_train_class0, frac_train_class1 ,frac_test_class0, \n",
    "                                  frac_test_class1, seed, DEBUG=False):\n",
    "    # This function samples randomly train and test sets. \n",
    "    # The function allows for:\n",
    "    # * UNBALANCED datasets (according to fractions specified)\n",
    "    # * Datasets can be only a part of the dataset (frac_train_class0 + frac_train_class0 <= 1.0, \n",
    "    # frac_train_class1+frac_test_class1 <= 1.0)\n",
    "    # This is usedful for example when wanting to train multiple SVMs on a smaller dataset, but still have \n",
    "    # enough class1 examples\n",
    "    # SVM is computaionally expensive O(n^2), so cutting on class0, can be useful\n",
    "    # df = expects dataframe\n",
    "    # col_class = the column containing the class (target) variable\n",
    "    # frac_* = fraction of records to include in train [0-1]. Must be float. \n",
    "    #          Summary of same-class fraction must be <= 1\n",
    "    #          frac_train_class0 + frac_test_class0 <= 1\n",
    "    #          frac_train_class1 + frac_test_class1 <= 1\n",
    "    # seed = random seed, to ensure randomness and repeatability of experiments\n",
    "    # returns: indices for train, and test (both classes), (returns 2 index types)\n",
    "\n",
    "    if (frac_train_class0+frac_test_class0 > 1.0):\n",
    "        sys.exit(\"Training sample fractions are larger than 1.0. Can't sample more than the complete dataset.\")\n",
    "    elif (frac_train_class1+frac_test_class1 > 1.0):\n",
    "        sys.exit(\"Test sample fractions are larger than 1.0. Can't sample more than the complete dataset.\")\n",
    "\n",
    "    train_class0 = df[df[col_class] == 0].sample(frac = frac_train_class0, replace = False, random_state = seed).index\n",
    "    train_class1 = df[df[col_class] == 1].sample(frac = frac_train_class1, replace = False, random_state = seed).index\n",
    "\n",
    "    ntest0 = int(frac_test_class0 * len(df[df[col_class] == 0]))\n",
    "    ntest1 = int(frac_test_class1 * len(df[df[col_class] == 1]))\n",
    "    test_class0 = df[df[col_class] == 0].loc[~df[df[col_class] == 0]\n",
    "                                             .index.isin(train_class0)].sample(n=ntest0, replace=False, \n",
    "                                                                               random_state = seed).index\n",
    "    test_class1 = df[df[col_class] == 1].loc[~df[df[col_class] == 1]\n",
    "                                             .index.isin(train_class1)].sample(n=ntest1, replace=False, \n",
    "                                                                               random_state = seed).index\n",
    "\n",
    "    if DEBUG:\n",
    "        num_tr0 = float(len(train_class0))\n",
    "        num_tr1 = float(len(train_class1))\n",
    "        num_ts0 = float(len(test_class0))\n",
    "        num_ts1 = float(len(test_class1))\n",
    "        total_tr = num_tr0 + num_tr1\n",
    "        total_ts = num_ts0 + num_ts1\n",
    "        print (\"TRAINSET:\")\n",
    "        print (\"\\tClass0: %d records (%.2f%% of total records in train)\" % (num_tr0, 100*num_tr0/total_tr))\n",
    "        print (\"\\tClass1: %d records (%.2f%% of total records in train)\" % (num_tr1, 100*num_tr1/total_tr))\n",
    "        print (\"\\tTotal records: %d\" % total_tr)\n",
    "        print (\"TESTSET:\")\n",
    "        print (\"\\tClass0: %d records (%.2f%% of total records in test)\" % (num_ts0, 100*num_ts0/total_ts))\n",
    "        print (\"\\tClass1: %d records (%.2f%% of total records in test)\" % (num_ts1, 100*num_ts1/total_ts))\n",
    "        print (\"\\tTotal: \" + str(total_ts))\n",
    "        print (\"Total rows used in datasets: \" + str(total_tr+total_ts))\n",
    "        print (\"Total rows in input file: \" + str(len(df)))\n",
    "\n",
    "        check = list(train_class0) + list(train_class1) + list(test_class0) +  list(test_class1) \n",
    "        if len(check) != len(set(check)):\n",
    "            warnings.warn(\"WARNING: Some indices overlap in the train/testsets. Please re-evaluate what you're trying to do\")\n",
    "        else:\n",
    "            print (\"Total unique records equal to the number of records (\"+str(len(check))+\")\")\n",
    "    \n",
    "    return train_class0.union(train_class1), test_class0.union(test_class1)\n",
    "\n",
    "\n",
    "def train_test_partial_dataset(df, col_class, frac_train, frac_test, seed, DEBUG=False):\n",
    "    # This function samples randomly train and test sets, so that both sets have similar percent of each class\n",
    "    # Diffrence from train_test_dataset() is that this has frac_train and frac_test so that fract_train+frac_test can be <= 1.\n",
    "    # This is usedful for example when wanting to train multiple SVMs on a smaller dataset, but still maintain the same size \n",
    "    # of testset.\n",
    "    # df = expects dataframe\n",
    "    # col_class = the column containing the class (target) variable\n",
    "    # frac_train, frac_train = fraction of records to include in train [0-1]. Must be float, and must add-up to \n",
    "    # <= 1.0 (i.e., the complete dataset)\n",
    "    # seed = random seed, to ensure randomness and repeatability of experiments\n",
    "    # returns: indices for train, and test (both classes), (returns 2 index types)\n",
    "    \n",
    "    if (frac_train+frac_test > 1):\n",
    "        sys.exit(\"Sample fractions are larger than 1.0. Can't sample more than the complete dataset.\")\n",
    "    \n",
    "    train_class0 = df[df[col_class] == 0].sample(frac = frac_train, replace = False, random_state = seed).index\n",
    "    train_class1 = df[df[col_class] == 1].sample(frac = frac_train, replace = False, random_state = seed).index\n",
    "\n",
    "    # number of records to sample\n",
    "    ntest0 = int(frac_test * len(df[df[col_class] == 0]))\n",
    "    ntest1 = int(frac_test * len(df[df[col_class] == 1]))\n",
    "    test_class0 = df[df[col_class] == 0].loc[~df[df[col_class] == 0]\n",
    "                                             .index.isin(train_class0)].sample(n=ntest0, replace=False, \n",
    "                                                                               random_state = seed).index\n",
    "    test_class1 = df[df[col_class] == 1].loc[~df[df[col_class] == 1]\n",
    "                                             .index.isin(train_class1)].sample(n=ntest1, replace=False, \n",
    "                                                                               random_state = seed).index\n",
    "\n",
    "    if DEBUG:\n",
    "        num_tr0 = float(len(train_class0))\n",
    "        num_tr1 = float(len(train_class1))\n",
    "        num_ts0 = float(len(test_class0))\n",
    "        num_ts1 = float(len(test_class1))\n",
    "        total_tr = num_tr0 + num_tr1\n",
    "        total_ts = num_ts0 + num_ts1\n",
    "\n",
    "        print (\"TRAINSET:\")\n",
    "        print (\"\\tClass0: \" + str(100*num_tr0/total_tr),)\n",
    "        print (\"\\tClass1: \" + str(100*num_tr1/total_tr))\n",
    "        print (\"\\tTotal: \" + str(total_tr))\n",
    "        print (\"TESTSET:\")\n",
    "        print (\"\\tClass0: \" + str(100*num_ts0/total_ts),)\n",
    "        print (\"\\tClass1: \" + str(100*num_ts1/total_ts))\n",
    "        print (\"\\tTotal: \" + str(total_ts))\n",
    "        print (\"Total rows used in datasets: \" + str(total_tr+total_ts))\n",
    "        print (\"Total rows in input file: \" + str(len(df)))\n",
    "        \n",
    "        check = list(train_class0) + list(train_class1) + list(test_class0) +  list(test_class1) \n",
    "        if len(check) != len(set(check)):\n",
    "            warnings.warn(\"WARNING: Some indices overlap in the train/testsets. Please re-evaluate what you're trying to do\")\n",
    "\n",
    "    return train_class0.union(train_class1), test_class0.union(test_class1)\n",
    "\n",
    "def train_test_dataset_indices(df, col_class, frac_train, seed):\n",
    "    # This function samples randomly train and test sets, so that both sets have similar percent of each class\n",
    "    # df = expects dataframe\n",
    "    # col_class = the column containing the class (target) variable\n",
    "    # frac_train = fraction of records to include in train [0-1]\n",
    "    # seed = random seed, to ensure randomness and repeatability of experiments\n",
    "    # returns: indices for train, and test (both classes), (returns 2 index types)\n",
    "    \n",
    "    train_class0 = df[df[col_class] == 0].sample(frac = frac_train, replace = False, random_state = seed).index\n",
    "    train_class1 = df[df[col_class] == 1].sample(frac = frac_train, replace = False, random_state = seed).index\n",
    "\n",
    "    test_class0 = df[df[col_class] == 0].loc[~df[df[col_class] == 0].index.isin(train_class0)].index\n",
    "    test_class1 = df[df[col_class] == 1].loc[~df[df[col_class] == 1].index.isin(train_class1)].index\n",
    "\n",
    "    return train_class0.union(train_class1), test_class0.union(test_class1)\n",
    "\n",
    "def train_test_datasets(df, df_class, frac_train, seed):\n",
    "    # returns x_train, y_train, x_test, y_test\n",
    "    train_class0 = df_class[df_class == 0].sample(frac = frac_train, replace = False, random_state = seed).index\n",
    "    train_class1 = df_class[df_class == 1].sample(frac = frac_train, replace = False, random_state = seed).index\n",
    "\n",
    "    test_class0 = df_class[df_class == 0].loc[~df_class[df_class == 0].index.isin(train_class0)].index\n",
    "    test_class1 = df_class[df_class == 1].loc[~df_class[df_class == 1].index.isin(train_class1)].index\n",
    "\n",
    "    idx_train = train_class0.union(train_class1)\n",
    "    idx_test = test_class0.union(test_class1)\n",
    "    \n",
    "    return df.iloc[idx_train], df_class.iloc[idx_train], df.iloc[idx_test], df_class.iloc[idx_test]\n",
    "\n",
    "\n",
    "def entropy(s):\n",
    "    res = 0\n",
    "    val, counts = np.unique(s, return_counts=True)\n",
    "    freqs = counts.astype('float')/len(s)\n",
    "    for p in freqs:\n",
    "        if p != 0.0:\n",
    "            res -= p * np.log2(p)\n",
    "    return res\n",
    "\n",
    "def mutual_information(y, x):\n",
    "    res = entropy(y)\n",
    "    # We partition x, according to attribute values x_i\n",
    "    val, counts = np.unique(x, return_counts=True)\n",
    "    freqs = counts.astype('float')/len(x)\n",
    "\n",
    "    # We calculate a weighted average of the entropy\n",
    "    for p, v in zip(freqs, val):\n",
    "        res -= p * entropy(y[x == v])\n",
    "    return res\n",
    "\n",
    "def dataset_classes(df, col_class):\n",
    "    classes = list(set(df[col_class])) \n",
    "    records_total = len(df)\n",
    "    print (\"The dataset contains \" + str(records_total))\n",
    "    for i in classes:\n",
    "        records_class = len(df[df[col_class] == i])\n",
    "        records_ratio = (float(records_class)/float(records_total))*100\n",
    "        print (\"\\tClass \" + str(i) + \" has \" + str(records_class) + \" (\"+str(records_ratio)+\"%)\"    )\n",
    "        \n",
    "def pct_rank_qcut(series, n):\n",
    "# qcut variant to avoid the \"unique-edges\" error\n",
    "    edges = pd.Series([float(i) / n for i in range(n + 1)])\n",
    "    f = lambda x: (edges >= x).argmax()\n",
    "    return series.rank(pct=1).apply(f)\n",
    "\n",
    "def dummify_variables(df):\n",
    "    \"\"\"Create dummy variables for all columns in df (assumes all are categorical)\"\"\"\n",
    "    \"\"\"Returns df of dummy variables\"\"\"\n",
    "    cols = df.columns.values\n",
    "    print (\"Number of columns before dummy-variables:\\t%s\" %(len(cols)))\n",
    "    for col in cols:\n",
    "        dummy_ranks = pd.get_dummies(df[col], prefix=col)\n",
    "        df = df.join(dummy_ranks)\n",
    "        # dropping the original categoric column\n",
    "        df = df.drop(col, 1)\n",
    "    \n",
    "    cols = df.columns.values\n",
    "    print (\"Number of columns after dummy-variables:\\t%s\" %(len(cols)))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
